* The service discovery pattern provides a stable endpoint at which clients of a service can access the instances providing the service.
* For this purpose, Kubernetes provides multiple solutions depending on whether the service consumers and producers are located on or off the cluster.

**Problem**
* Interaction between an application and other service can be initiated internally or via external stimulus.
* Examples of internally initiated interactions:
    * An application within a Pod that reaches a file server and starts consuming files.
    * Or an application connecting to a message broker/relational database/key-value store and starts reading or writing data.
* For internally initiated interactions, we don't need any additional Kubernetes setup.

* The more common-usecase of Kubernetes workloads occurs when we have long running services expecting external stimulus, most commonly in the form of incoming HTTP connections from other Pods within the cluster or external systems.

* In these cases, *service consumers need a mechanism for discovering Pods that are dynamically placed by the scheduler and sometimes dynamically scaled up and down*.

* It would be a significant challenge if we had to perform *tracking, registering and discovering endpoints of dynamic Kubernetes pods ourselves*. Service discovery pattern solves this problem.

**Solution**
* **Pre Kubernetes Era: Client-side Service Discovery**
    * Most common method of service-discovery was through client-side discovery.
    * When a consumer service had to call another service that might be scaled to multiple instances, the service consumer would have a discovery agent capable of looking at a registry and then choosing one to call.
    * Classically that would be done either via an embedded agent within the consumer(eg. Zookeeper, Consul, Robbin) or a colocated process like Prana.

* **Post Kubernetes Era: Server-side Service Discovery**
    * Many of the non-functional responsibilities of distributed systems such as *placement, health check, healing and resource isolation* are moving into the platform and so is service discovery.
    * In the Kubernetes, all of it happens behind the scenes so that a service consumer calls a fixed virtual Service endpoint that can dynamically discover Service instances implemented as Pods.

* Multiple mechanisms can be used to implement service discovery pattern depending on whether the service consumer is within or outside the cluster and whether the service provider is within or outside the cluster.

**Internal Service Discovery**
* Kubernetes Service object provides a constant and stable entry point for a collection of Pods offering the same functionality. The easiest way is via kubectl expose which creates a Service for a Pod or multiple Pods of a Deployment or ReplicaSet.

* Example 12.1
    * selector:
        app: random-generator
        * Only pods maching this selector will be part of the service.
    * port: 80 ==> Port over which the service can be contacted.
    * targetPort: 8080 ==> Port on which the Pods are listening.

* Once a Service is created, it gets a clusterIP assigned that is accessible only from within the Kubernetes cluster.
* The clusterIP remains unchanged as long as the service definition exists.

How can other applications within the cluster figure out what that dynamically allocated IP is?

* Method1. *Discovery through environment variables*
    * When Kubernetes starts a Pod, its environment variables get populated with the details of all Services that exist up to that moment.
    * The application running that Pod would know the name of the Service it needs to consume and can be coded to read these environment variables.
    * Only issue: temporal dependency on Service creation. Environment variables are only for Pods that started after the Service creation. This requires either the Service creation before Pod creation or Pods to be restarted.

    `
        RANDOM_GENERATOR_SERVICE_HOST=10.109.72.32
        RANDOM_GENERATOR_SERVICE_PORT=8080
    `

* Method2. *Discovery through DNS Lookup*
    * Kubernetes runs a DNS server that all Pods are configured to use.
    * When a new Service is created, it automatically gets a new DNS entry that all Pods can start using.
    * Assuming a client knows the name of the service it wants to access. It can reach the Service by a fully-qualified domain name (FQDN) such as random-generator.default.svc.cluster.local.
        * random-generator ==> Name of the service.
        * default ==> Name of the namespace.
        * svc ==> Indicates that it is a service resource.
        * cluster.local ==> Cluster specific suffix.
        * We can omit the cluster suffix and namespace if accessing the service from same namespace.
    * The DNS discovery mechanism doesn't suffer from the drawbacks of the environment-variable-based mechanism. However we may still need environment based look up for port number.

* Characteristics of the Service with type: ClusterIP

* *Multiple ports*
    * A single Service definition can support multiple source and target ports.

* *SessionAffinity*
    * For each request, the Service picks a Pod randomly to connect to by default.
    * This can be changed with SessionAffinity: ClientIP, which makes all request originating from the same ClientIP stick to the same Pod.
    * Note: Kubernetes service performs L4 transport layer load balancing and cannot looks into the network packets and perform application-level load balancing such as HTTP cookie-based session affinity.

* *Readiness Pobes*
    * If a Pod has defined readiness checks and they are failing, the Pod is removed from the list of service endpoints to call even if the label selector matches the Pod.

* *Virtual IP*
    * The network IP address does not correspond to any network interface and doesn't exist in reality.
    * It is the kube-proxy that runs on every node that picks this new Service.
    * kube-proxy updates the iptables of the node with rules to catch the network packets destined for this virtual IP address and replaces it with a selected Pod IP address.
    * The rules in the IP tables do not add any ICMP rules, but only the protocol specified in the service definition such as TCP or UDP.
    * Hence it is not possible to ping the IP address of the service as that operation using ICMP protocol.
    * It is still possible to access the service IP address via TCP.

* *Choosing ClusterIP*
    * Field .spec.clusterIP.
    * While not recommended, it can turn out to be handy when dealing with legacy applications configured to use a specific IP address, or there is an existing DNS entry that we wish to reuse.

**Manual Service Discovery**
* When we create a Service with .spec.selector, Kubernetes tracks the list of matching and ready-to-serve Pods in the list of endpoint resources. `kubectl get endpoints random_generator`

* Instead of redirecting connections to Pods within the cluster, we could also redirect connections to external IP addresses and ports.
    * We can do that by omitting the selector definition and manually creating Endpoints resource. example 12.3(omitting selector), 12.4 (Endpoints resource).
    * .metadata.name should be same for both Service, Endpoints resource.
    * The service is accessible only within the cluster and can be accessed through one of the similar ways: env variables, DNS lookup.

* Example 12.5 => Service with external destination. type: ExternalName
    * The service definition maps to the content pointed by externalName using DNS only.
    * It is a way of creating an alias for an external endpoint using DNS CNAME rather than going through the proxy with an IP address.

**Service Discovery From Outside The Cluster**
**Method1: type: NodePort**
* Example 12.6. 
    * As earlier:
        1. Service serves Pods that match the selector `app: random generator`.
        2. Accepts connection on port 80 on the virtual IP address.
        3. Routes connections to port 8080 of the selected Pods.
    * In addition to all of that, the definition also reserves port 30036 on all the nodes and forwards incoming connections to the Service.
    * This reservation makes the service accessible both internally (through the virtual IP address) as well as externally (through a dedicated port on every node).
    * `type: NodePort` results in port being open on all nodes.
* Pros and Cons of NodePort approach:
    * *Firewall rules*
        * Since this method opens a port on all nodes, we may have to configure additional firewall rules to help external clients access the node ports.

    * *Node Selection And Load Balancing*
        * If a node is not available, it is the responsibility of the client application to connect to another healthy node.
        * Hence it is better to place a load balancer in front of the nodes to perform failovers or retries.
    
    * *Pods selection*
        * When a client opens a connection to a node, it is routed to one of the Pod. The Pod may or may not be on the same node.
        * It is possible to avoid this extra hop and force Kubernetes to pick a Pod on the node where the connection was opened by adding: `externalTrafficPolicy: Local` to the service definition.
        * With this approach we'll have to ensure that there are Pods placed on every node eg. by using DaemonSets or sharing the info to the client that which healthy nodes have Pods assigned to them. 

**Method2: type: Load Balancer**
* In addition to creating a regular service and opening a port on every node, it also *exposes the service externally using a cloud provider's load balancer*.
* This type of service only works when the cloud provider has Kubernetes support and provisions a load balancer.
* We can assign the clusterIP and loadBalancerIP in the spec field and the ingress ip in the .status.ingress.ip field.
* Some of the implementation are cloud provider specific. Some will allow defining the load balancer address and some won't. Some will preserve the source address and some will replace it with load-balancer address.

**Headless Services**
    * We don't request a dedicated IP address. (`clusterIP: None`).
    * We will see their detailed usage while reading Stateful Services.

**Application Layer Service Discovery**
* Unlike the other mechanisms, Ingress is not a service type, but a separate Kubernetes resource.
* Common Ingress features:
    1. HTTP-based access to Services through externally reachable URLs
    2. Load Balancing
    3. SSL termination
    4. Name-based virtual hosting

* For ingress to work, the cluster must have one or more Ingress controllers running.
* The real power of Ingress comes from reusing a single external load balancer and IP to service multiple Services and reduce the infrastructure costs. This is generally done via URI path or hostname routing. We can set up such rules in Ingress.
* Example 12.9 => Redirect every request to random-generator except /cluster-status which goes to another Service.

* Since every Ingress controller implementation is different, a controller may require addition configuration via annotations. Eg: `nginx.ingress.kubernetes.io/rewrite-target: /`

* Ingress is most useful for exposing multiple services under the same IP address and when all services use the same L7 (typically HTTP) protocol.