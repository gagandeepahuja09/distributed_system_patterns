* Batch Job pattern is suited for managing isolated atomic units of work.
* It is based on Job abstraction, which runs short lived Pods reliably until completion on a distributed environment.

**Problem**
* Different ways of creating Pods.
* *Bare or Unmanaged or Naked Pod*
    * The node which a Pod is running on fails, it is not restarted.

* *ReplicaSet*  
    * This controller is used for creating and managing the lifecycle of Pods expected to run *continuously*. Eg. to run a web server container.
    * It maintains a stable set of replica Pods running at any given time.
    * It *guarantees the availability of a specified number of identical Pods*.

* *DaemonSet*
    * A controller for running a single Pod on every node.
    * Typically used for managing platform capabilities like monitoring, log aggregation, storage containers, etc.

* All of the above are used for long-running processes that are not meant to stop after some time.

* However in some cases, there is a need to perform a predefined finite unit of work and then shut down the container. For this task, Kubernetes provides the Job resource.

**Solution**
* A Kubernetes Job is similar to ReplicaSet in that both of them create one or more Pods are ensure that they run successfully.
* Difference: Once the expected no. of Pods terminate successfully, the Job is considered complete and no additional Pods are started.
* Example 7.1
    * .spec.completions=5 => Job should run 5 Pods to completion which all must succeed.
    * .spec.parallelism=2 => 2 Pods can run in parallel.
    * .spec.template.spec.restartPolicy=OnFailure. => Specifying the restartPolicy is mandatory for a Job. 
    * The default restartPolicy in ReplicaSet is Always which makes sense for long-running processes that must always be kept running.
    * Always is not allowed for Job as restartPolicy and the only options are Never or OnFailure.

* Why bother creating a Job to run a Pod only once instead of using bare Pods? It provides many reliability and scalability options.
    1. A Job is not a ephemeral in-memory task, but a persisted one that survives cluster restarts.
    2. When a Job is completed, it is not deleted but kept for tracking purposes. The Pods are also not deleted but only terminated.
    3. .spec.completions for doing a Job multiple times and .spec.parallelism for scalability.
    4. If a node fails or when the Pod is evicted for some reason while still running, the scheduler places the Pod on a healthy node and reruns it.