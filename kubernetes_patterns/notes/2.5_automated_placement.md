* Automated placement is the core function of the Kubernetes scheduler for assigning new Pods to nodes satisfying container resource requests and honoring *scheduling policies*.
* This patterns describes:
    * The principles of Kubernetes' scheduling algorithm.
    * The way to influence the placement decision from the outside.

**Problem**
* Containers have dependencies among themselves, dependencies to nodes, and resource demands, all of that changes over time too.
* The resources available on a cluster also vary over time, through shrinking or extending the cluster, or by having it consumed by already placed containers.
* The way we place containers also impacts the availability, performance, and capacity of the distributed systems.
* All of this make scheduling containers both important and difficult(impossibe to do manually).

**Solution**
* We'll cover:
    * Main scheduling control mechanisms.
    * Driving forces that affect the placement.
    * Why to choose one or the other option and the resulting consequence.
* Similar to other Kubernetes components (API server, Kubelet), Scheduler can be run in isolation or not used at all.

* At a very high level, its main operation is to retrieve each newly created Pod definition from API server and assign it to a node.
* It finds a suitable node for every Pod (as long as there is such a node), whether that is for:
    * initial application placement
    * scaling up
    * moving an application from an unhealty node to a healthier one.
* It does so by considering runtime dependencies, resource requirements, and guiding policies for high availability by spreading Pods horizontally and also colocating Pods nearby for performance and low-latency interactions.

* For a scheduler to do its job correctly, it needs nodes with available capacity, and containers with declared resource profiles and guiding policies in place.

**Available Node Resources**
* Kubernetes cluster needs to have nodes with enough resource capacity to run new Pods. Node capacity for a node only dedicated to Kubernetes:
`
    Allocatable [capacity for application pods] = 
        Node Capacity [available capacity on node]
        - Kube-Reserved [Kubernetes daemons like kubelet, container runtime]
        - System-Reserved [OS system daemons like udev, sshd]
`

**Container resource demands**
* Container should have their runtime dependencies and resource requirements defined.
* It boils down to having containers that declare their resource profiles (with request and limit) and environment dependencies such as storage or ports.
* Only then are Pods sensibly assigned to nodes and can run without affecting each other during peak times.

**Placement Policies**
* The scheduler has a default set of placement or predicate policies which are good enough for most cases and can be overridden during scheduler startup.
* Scheduler policies and custom schedulers can be defined only by an administrator as part of the cluster configuration.

* *Predicates* are rules that filter out unqualified nodes. Eg: *PodFitsHostPort* schedules Pods to request certain fixed host ports only on the nodes that have this port available.

* *Priorities* are rules that sort available nodes according to preferences. Eg. *LeastRequestedPriority* gives nodes with fewer requested resources a higher priority.

* In addition to configuring the policies of the default scheduler, it is also possible to run multiple schedulers and allow Pods to specify which scheduler to place them by specifying the field .spec.schedulerName.

**Scheduling Process**
* Refer diagram.
* In most cases, it is better to let the scheduler do the Pod-to-node assignment and not micromanage the placement logic. However in some cases, we may want to force the assignment of a Pod to a specific node or a group of nodes.
* It can be done using a node selector.
* .spec.nodeSelector is Pod field and specifies a map of key-value pairs that must be present as labels on the node for the node to be eligible to run the Pod. Eg. only nodes labelled with disktype=ssd will be eligible to run the Pod.
* Apart from custom labels, we can use some of the default labels that are present on every node. Every node has a unique kubernetes.io/hostname label that can be used to place a Pod on a node by its hostname. Other useful labels: OS, architecture, instance-type.

**Node Affinity**
* Node affinity is a generalization of the node selector approach.
* It allows specifying rules as either required or preferred.
* Required rules: must be met.
* Preferred rules: only increase the weight for the matching nodes.
* The node affinity feature expands the type of constraints we can express by making the language more expressive with operators: In, NotIn, Exists, DoesNotExist, Gt or Lt.

* 6.4 example:
    * *requiredDuringSchedulingIgnoredDuringExecution*: Hard requirement that the node must have more than 3 cores.
    * *matchExpressions*: Match on labels.
    * *preferredDuringSchedulingIgnoredDuringExecution*: For every node, the sum of all weights for matching selectors is calculated, and the highest-valued node is chosen, as long as it matches the hard requirement.
    * *matchFields*: Match on a field specified as jsonpath. Note that only In and NotIn are allowed as operators and only one value is allowed to be given in the list of values.

**Pod Affinity and Antiaffinity**
* Pod affinity allows expressing dependencies among pods to dictate where a pod should be placed relative to other Pods.
* It helps in the following: 
    1. Expressing how Pods should be spread to acheive high availability.
    2. Packing and colating together to improve latency.
* The suffix IgnoredDuringExecution exists for future extensibility reasons.
* podAffinity is not limited to nodes and can express express rules at multiple topology levels.
* Using the topologyKey and labelSelector -> matchLabels fields, we can form more fine-grained rules which are combination of domains like node, rack, cloud provider zone and region.

* Example 6.5
    * podAffinity => labelSelector
        * Label selector to find the pods to be colocated with.
    * The podAffinity is a required rule, hence a pod with value confidential=high is required in that node for it to be running in the same node. The node should also be carrying a label of security-zone. Note: this can also be for other domains like rack, cloud provider zone and region.
    * podAntiAffinity ==> rule describing that the pod should not be placed (but could be placed) in a node where a pod with label confidential=none is already placed.

**Taints and Tolerations**
* They allow a node to control which Pods should or should not be placed on them.
* A taint is a characteristic of a node and when it is present, it prevents Pods from scheduling onto the node unless the Pod has a toleration to the taint.

* A taint is added to a node by using kubectl: kubectl taint nodes master node-role.kubernetes.io/master="true".

* Example 6.6 and 6.7: 
    * On production clusters, the taint is set on the master node to prevent scheduling of Pods on the master. 
    * The toleration in exampe 6.7 allows the Pod to be installed on the master nevertheless.
    * effect in the toleration example can be empty also, which will mean that it will apply to all effects.

* effect=NoSchedule: Hard taints that prevent scheduling on a node.
* effect=PreferNoSchedule: Soft taints that avoid scheduling on a node.
* effect=NoExecute: Can evict already running Pods from a node.

* Taints and toleration allow for complex use-cases like: 
    * Having dedicated nodes for an exclusive set of Pods.
    * Force evictiong of Pods from problematic nodes by tainting those nodes.