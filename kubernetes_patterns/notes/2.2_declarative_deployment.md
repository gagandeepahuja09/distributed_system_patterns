* Kubernetes' Deployment resource abstraction *encapsulates the upgrade and rollback of a group of containers* and makes its *execution a repeatable and automated activity*.

**Problem**
* Updating a service to a next version involves activities such as:
    1. Starting the new version of the Pod.
    2. Stopping the old version of a Pod gracefully.
    3. Waiting and verifying that it has launched successfully.
    4. Rolling it back to all of the previous versions in case of a failure.

* These activities are performed by either allowing some downtime but no running concurrent service versions, or with no downtime, but increased resource usage due to both version of the service running during the update process.

* Performing these step manually can lead to human errors, and scripting properly can require a significant amount of effort, both of which can quickly turn the release process into a bottleneck.

**Solution**
* Luckily Kubernetes has automated this step as well using Deployment object.
* We can describe how our application should be updated using different strategies and tuning the various aspects of the update process.

* To do its job effectively, the scheduler requires sufficient resources on the host system, appropriate placement policies, and containers with adequately defined resource profiles.

* Similarily for a Deployment to do its job effectively, it expects the containers to be good cloud-native citizens. For this to work as expected:
    1. The containers themselves usually listen and honor lifecycle events such as SIGTERM.
    2. They are provide health check endpoints which indicate whether they started successfully.
* If a container covers these 2 areas accurately, the platform can cleanly shutdown old containers.

**Imperative Rolling Updates With kubectl Are Depreceated**
* kubectl rolling-update Drawbacks:
    * Rather than describing the desired end-state, it issues commands to get the system to the desired state.
    * The whole orchestration logic for replacing the containers and the Replication-Controllers is performed by kubectl, which monitors and interacts with the API server behind the scenes while the update process happens, moving an inherent server-side responsibility to the client.
    * We may need more than one command to get the system into the desired state. These commands must be automated and repeatable in different environments.
    * Somebody else may override our changes with time.
    * The update process has to be documented and kept up-to-date while the service evolves.
    * The only way to find out what we have deployed is by checking the condition of the system. Sometimes it might not match the desired as per documentation, in which case we have to correlate it.

**Rolling Deployment**
* Behind the scenes, the Deployment creates a ReplicaSet that supports set-based label selectors.
* Deployment strategies: RollingUpdate(default) and recreate.

* replicas: 3
    * We need more than 1 replica for a rolling update to make sense.
* strategy:
    rollingUpdate: 
        maxSurge: 1
    * No. of pods that can be run temporarily in addition to the replicas specified during the update. Maximum possible replicas during 
* rollingUpdate:
    maxUnavailable: 1
    * Here, it could be that only 2 pods are available at a time during the update.

* With Deployment, it is possible to control the rate of a new container rollout. The Deployment object allows us to control the range of available and excess Pods through maxSurge and maxUnavailable fields.

* To trigger a declarative update, we have 3 options:
    1. Replace the deployment with whole version's deployment with kubectl replace.
    2. Patch (kubectl patch) or interactively edit (kubectl edit) the Deployment to set the new container image of the new version.
    3. Use kubectl set image to set the new image in deployment.

* How declarative deployment solves all the imperative deployment problems:
    1. Deployment is a Kubernetes resource object whose status is managed by Kubernetes internally. The whole update process is performed on the server without any client interaction.
    2. Desired state vs steps.
    3. The Deployment definition is an executable object, tried and tested on multiple environments before reaching production.
    4. The update process is also wholly recorded, and versioned with options to pause, continue and roll back to previous versions.

**Fixed Deployment**
* RollingUpdate helps with zero downtime but the side effect of the approach is that two versions of the container are running at the same time.
* This can cause issues when the update process has introduced backward incompatible changes in the APIs and the client is not capable of dealing with them.
* We use Recreate strategy for this. It has the effect of setting maxUnavailable to the number of declared replicas.
* This means that it first kills all containers from the current version and then starts all new containers simultaneously when the old ones are evicted.

**Blue-Green Deployment**
* Focuses on minimizing downtime and reducing risk.
* We can use the Deployment primitive as a building block, together with other Kubernetes primitives to build this more advanced release strategy of a Blue-green deployment.
* It needs to be done manually if no extensions like a Service Mesh or Knative is used.
* It works by create a 2nd Deployment with the latest version of the containers not serving any requests yet (Green).
* At this stage, The old Pod replicas (Blue) from the original Deployment are still running and serving live requests.

* Once we are confident that the new version of the Pods is healthy and ready to handle live requests, we switch the traffic from old Pod replicas to the new replicas.
* This activity in Kubernetes can be done by *updating the Service selectors* to match the new containers (tagged as green).
* Once the green containers handle all the traffic, the blue containers can be deleted and the resources freed for future Blue-green deployments.

* Pros:
    * Only one version of the application serving requests, which reduces the complexity of handling multiple concurrent versions by the Service consumers.
* Cons:
    * Requires 2X application capacity while both blue and green containers are up and running.
    * There can be significant complications with long-running processes and database state drifts during the transitions.

**Canary Release**
* A way to softly deploy a change into production.
* It reduces the risk of introducing a new version by replacing a small subset of instance with new ones.
* This technique can be implemented by creating a new ReplicaSet for the new container version(preferably using a Deployment) with a small replica count that can be used as the Canary instance.
* We do the rolling deployment only on the canary ReplicaSet first.