* Kubernetes' Deployment resource abstraction *encapsulates the upgrade and rollback of a group of containers* and makes its *execution a repeatable and automated activity*.

**Problem**
* Updating a service to a next version involves activities such as:
    1. Starting the new version of the Pod.
    2. Stopping the old version of a Pod gracefully.
    3. Waiting and verifying that it has launched successfully.
    4. Rolling it back to all of the previous versions in case of a failure.

* These activities are performed by either allowing some downtime but no running concurrent service versions, or with no downtime, but increased resource usage due to both version of the service running during the update process.

* Performing these step manually can lead to human errors, and scripting properly can require a significant amount of effort, both of which can quickly turn the release process into a bottleneck.

**Solution**
* Luckily Kubernetes has automated this step as well using Deployment object.
* We can describe how our application should be updated using different strategies and tuning the various aspects of the update process.

* To do its job effectively, the scheduler requires sufficient resources on the host system, appropriate placement policies, and containers with adequately defined resource profiles.

* Similarily for a Deployment to do its job effectively, it expects the containers to be good cloud-native citizens. For this to work as expected:
    1. The containers themselves usually listen and honor lifecycle events such as SIGTERM.
    2. They are provide health check endpoints which indicate whether they started successfully.
* If a container covers these 2 areas accurately, the platform can cleanly shutdown old containers.

**Imperative Rolling Updates With kubectl Are Depreceated**
* kubectl rolling-update Drawbacks:
    * Rather than describing the desired end-state, it issues commands to get the system to the desired state.
    * The whole orchestration logic for replacing the containers and the Replication-Controllers is performed by kubectl, which monitors and interacts with the API server behind the scenes while the update process happens, moving an inherent server-side responsibility to the client.
    * We may need more than one command to get the system into the desired state. These commands must be automated and repeatable in different environments.
    * Somebody else may override our changes with time.
    * The update process has to be documented and kept up-to-date while the service evolves.
    * The only way to find out what we have deployed is by checking the condition of the system. Sometimes it might not match the desired as per documentation, in which case we have to correlate it.